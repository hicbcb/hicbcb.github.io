[{"categories":["Research Note"],"contents":"How to install gurobi python and get academic licence (Windows) Install gurobi \u0026amp; get license 1. Log in with your academy email (https://www.gurobi.com/downloads/) and download Gurobi Optimizer. Chose the corresponding version 2. Access the licence (Academy) Use the academic License:\ncopy your key (in this example is \u0026quot; grbgetkey 53f2fe08-fade-11eb-8a57-0242ac130004 \u0026ldquo;)\nActivate Gurobi 1. Open Your cmd or WSL, and paste your Gurobi licence key  grbgetkey 53f2fe08-fade-11eb-8a57-0242ac130004 copy the file path \u0026ldquo;C:\\Users\\USER\\y\\gurobi.lic\u0026rdquo;\n If it shows the message \u0026ldquo;Unable to contact key server\u0026rdquo;, it means failure of activation. You should redo the previous steps.\n 2. Setting Environment Variable Windows —\u0026gt; Setting —\u0026gt; About —\u0026gt;Advance system setting —\u0026gt; Environment variable\nSystem Variable —\u0026gt; add a system variable with name \u0026ldquo;GRB_LICENSE_FILE\u0026rdquo; and value \u0026ldquo;C:\\Users\\USER\\y\\gurobi.lic\u0026rdquo; (your file path).\nPaste \u0026ldquo;the file path \u0026ldquo;C:\\Users\\USER\\y\\gurobi.lic\u0026rdquo; on variable value.\nInstall Gurobi into python library  Open terminal in the file which is installed Gurobi  Input the command :\n python setup.py install  Test in python from gurobipy import *  Reference 如何在python中安装Gurobi（详细教程）_Book-bei的博客-CSDN博客_gurobi python\nPython - gurobi\nThe optimization model\nSimple Python Example\nMy notion version\n","permalink":"https://hicbcb.github.io/blog/gurobi_install/","tags":["Gurobi"],"title":"Install gurobi \u0026 get license"},{"categories":["Reading Note"],"contents":"credit:　Deep Learning with Pytorch\nch 7 - Telling birds from airplanes:Learning from images 這章幾個focus的重點:\n 建立前饋神經網絡 使用Datasets和DataLoaders加載資料 了解分類損失   第六章提到了通過\u0026quot;梯度下降\u0026ldquo;深入學習的內部機制，還有PyTorch提供的用於構建模型和優化模型的工具。 為此，這裡使用了一個只有一個輸入和一個輸出的簡單回歸模型，能夠一目了然 (但這只是borderline exciting(???))\n  繼續建立神經網絡基礎\n  將注意力轉移到圖像上(圖像識別)\n  通過一個簡單的神經網絡（如上一章中定義的）逐步解決一個簡單的圖像識別問題\n  使用更廣泛的微小圖像資料集(A dataset of tiny images) (不是微小的數字資料集)\n   7-1 A dataset of tiny images 圖像識別的最基本的dataset之一：MNIST(手寫數字識別資料集) 先下載這章需要的資料集準備使用: CIFAR-10 (和它的同級CIFAR-100一樣，已成為十年來Computer Vision的經典之作)\nCIFAR-10 Image samples from all CIFAR-10 classes 對應的分類標記：飛機（0），汽車（1），鳥（2），貓（3），鹿（4），狗（5），青蛙（6），馬（7），輪船（8）和卡車（9）。\nCIFAR-10有60,000個微小的32×32色（RGB）圖像\n如今CIFAR-10被認為過於簡單，無法開發或驗證新研究\n但它可以很好地滿足學習目的\n  使用該torchvision模塊自動下載資料集 將其加載為PyTorch Tensor的集合   1 \u0026ndash; 下載CIFAR-10資料集 # s = \u0026#34;python syntax highlighting\u0026#34;\rfrom torchvision import datasets\r# 導入torchvision模組中的 datasets\rdata_path = \u0026#39;../data-unversioned/p1ch7/\u0026#39;\r# 下載資料的位置路徑\rcifar10 = datasets.CIFAR10(data_path, train=True, download=True)\r# 實例化資料集 (給training data用)\r# 如果資料不存在，用TorchVision下載\rcifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\r# 當train=False,這段再次下載資料，但這次是給**驗證資料集**用 # (再次的下載資料集是必要的)\r# 第一個參數: 下載資料的位置(data_path)\r# 第二個參數: 指定我們對訓練集還是驗證集感興趣\r# 第三個參數: 如果在第一個參數指定的位置找不到資料，是否允許PyTorch下載資料。  cifar10實例的方法解析順序將其作為基類包括在內：\n# In[4]:\rtype(cifar10).__mro__ datasets子模塊提供固定的最受歡迎的CV資料集。 ( 例如:MNIST, Fashion-MNIST, CIFAR-100, SVHN, Coco, Omniglot\u0026hellip;)\n# Out[4]:\r(torchvision.datasets.cifar.CIFAR10,\rtorchvision.datasets.vision.VisionDataset,\rtorch.utils.data.dataset.Dataset,\robject) 在每種情況下，資料集均作為torch.utils.data.Dataset的子類return。  2 \u0026ndash; The Dataset class tags: PyTorch Dataset 物件的概念\u0026ndash;\u0026gt;它不一定包含資料，但可以通過__len__ 和 __getitem__對其進行統一訪問。 實現所需的物件兩種方法：  __len__ : 回傳資料集中的項目數 __getitem__ :回傳由樣本及其相應標籤（整數index）組成的項目  # In[5]:\rlen(cifar10) 實際上，當Python object配備了__len__方法時，我們可以將其作為參數傳遞給len這個內建函數：\n# Out[5]: 50000 同樣，dataset已配備__getitem__方法\n因此可以使用標準下標為tuples和list建立索引以訪問單個項\n# In[6]:\r# 在這裡獲得一個帶有預期輸出的 PIL圖像\r# (Python Imaging Library, the PIL package)\r# 一個值為的整數1，對應於類別“汽車”\rimg, label = cifar10[99] # why is 99??\r# 因為CIFAR-10資料集中的第99張圖像：一輛汽車\rimg, label, class_names[label] # Out[6]:\r(\u0026lt;PIL.Image.Image image mode=RGB size=32x32 at 0x7FB383657390\u0026gt;,\r1,\r\u0026#39;automobile\u0026#39;) data.CIFAR10資料集中的樣本是RGB PIL圖像的實例。可以立即將其繪製：\n# In[7]:\rplt.imshow(img) #繪圖\rplt.show() 輸出為CIFAR-10資料集中的第99張圖像：一輛汽車  3 \u0026ndash; Dataset transforms 將PIL圖像轉換為Tensor: torchvision.transforms\ntorchvision.transforms 定義一組可組合、類似於函數的物件，可以將這些物件作為參數傳遞給torchvision諸如的資料集datasets.CIFAR10(...)，並在資料加載後回傳之前由__getitem__對資料執行轉換。\n# In[8]:\rfrom torchvision import transforms\rdir(transforms) # Out[8]:\r[\u0026#39;CenterCrop\u0026#39;,\r\u0026#39;ColorJitter\u0026#39;,\r...\r\u0026#39;Normalize\u0026#39;,\r\u0026#39;Pad\u0026#39;,\r\u0026#39;RandomAffine\u0026#39;,\r...\r\u0026#39;RandomResizedCrop\u0026#39;,\r\u0026#39;RandomRotation\u0026#39;,\r\u0026#39;RandomSizedCrop\u0026#39;,\r...\r\u0026#39;TenCrop\u0026#39;,\r\u0026#39;ToPILImage\u0026#39;,\r\u0026#39;ToTensor\u0026#39;,\r...\r] ToTensor(其中一種transform)  可將NumPy array和PIL圖像轉換為tensor tensor輸出維度:C × H × W (channel, height, width; just as we covered in chapter 4).  # In[9]:\rto_tensor = transforms.ToTensor()\rimg_t = to_tensor(img)\rimg_t.shape # C × H × W (channel, height, width) # Out[9]: torch.Size([3, 32, 32]) #C × H × W   # 該圖像已變成 3×32×32 tensor # 因此變成了3通道（RGB）32×32圖像 tags: 注意，label什麼都沒發生；它仍然是整數。 # In[10]:\r# 可以直接通過變數轉換到dataset.CIFAR10\rtensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\rtransform=transforms.ToTensor()) 看訪問資料集的元素類型:\n# In[11]:\rimg_t, _ = tensor_cifar10[99]\rtype(img_t) # Out[11]:  torch.Tensor  # 此時，訪問資料集的元素將回傳tensor，而不是PIL圖像 The shape has the channel as the first dimension, while the scalar type is float32:\n# In[12]:\rimg_t.shape, img_t.dtype # Out[12]: (torch.Size([3, 32, 32]), torch.float32)   原始PIL圖像中的值範圍從0到255（每通道8 bits）\n  ToTensor turns the data into a 32-bit floating-point per channel, scaling the values down from 0.0 to 1.0 :\n  # In[13]:\rimg_t.min(), img_t.max() # Out[13]: (tensor(0.), tensor(1.)) 驗證是否得到了相同的圖像：\n# In[14]: plt.imshow(img_t.permute(1, 2, 0)) plt.show() # Out[14]: \u0026lt;Figure size 432x288 with 1 Axes\u0026gt; tags: 注意如何使用permute更改軸的順序(從C×H×W到H×W×C)與Matplotlib預期的相匹配  4 \u0026ndash; Normalizing data (資料正規化) 透過選擇linear activation functions (that are linear around 0 plus or minus 1 (or 2))，將資料保持在相同範圍內，意味著神經元更有可能具有非零梯度(nonzero gradients and)，因此能更快學習。\n同樣的，正規化每個channel以讓它具有相同的分佈，能確保可以使用相同的學習速率，通過梯度下降來混合和、更新channel的訊息。(參見ch4, 5)\n為了使每個通道的平均值(means) 和單一標準差(standard deviation) = 0，可以用以下轉換計算整個資料集中，每個channel的平均值和標準差：\nv_n[c] = (v[c] - mean[c]) / stdev[c]  # The values of \u0026#34;mean\u0026#34; and \u0026#34;stdev\u0026#34; must be computed offline  # (they are not computed by the transform) (上式是transforms.Normalize在做的運算)\n由於CIFAR-10資料集很小，因此我們將能夠在記憶體中完全對其進行操作。\n# In[15]:\r#沿著額外的維度堆疊資料集，回傳的所有tensor：\rimgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\rimgs.shape # Out[15]: torch.Size([3, 32, 32, 50000])  計算每個平均值(mean):  # In[16]:\rimgs.view(3, -1).mean(dim=1)\r# Out[16]:\rtensor([0.4915, 0.4823, 0.4468])  計算標準差(standard deviation):  # In[17]:\rimgs.view(3, -1).std(dim=1) # Out[17]: tensor([0.2470, 0.2435, 0.2616])  用以上得到的數值初始化 the Normalize transform:  # In[18]:\rtransforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)) # Out[18]: Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))  連接之後的ToTensor變換(用transforms.Compose):  # In[19]:\rtransformed_cifar10 = datasets.CIFAR10(\rdata_path, train=True, download=False,\rtransform=transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize((0.4915, 0.4823, 0.4468),\r(0.2470, 0.2435, 0.2616))\r])) 請注意在這一點上，畫從資料集繪製的圖像，不會提供真實圖像的真實呈現：\n# In[21]:\rimg_t, _ = transformed_cifar10[99]\rplt.imshow(img_t.permute(1, 2, 0))\rplt.show() # out[21]: tags: 正規化後的隨機CIFAR-10圖像 我們得到的重新歸一化的紅色汽車\n 因為正規化改變了RGB levels超出0.0~1.0範圍，並更改了通道的整體幅度(magnitudes of the channels)。 所有資料仍然存在，只是Matplotlib將其呈現為黑色。   7.2 Distinguishing birds from airplanes(從飛機上辨別鳥類)  故事: 作者賞鳥俱樂部的朋友Jane在機場南面的樹林中，架一組攝影機，當物體進入鏡時，攝影機會保存鏡頭影像並將其上傳到俱樂部的部落格，供大家即時觀鳥。 問題: 從機場進出的許多飛機觸發攝影機，因此Jane需花大量時間從部落格中刪除飛機的圖片。 她需要一個自動系統：不需人工刪除，而是需要一個神經網絡。  tags: 眼前的問題\u0026ndash;通過訓練神經網絡來幫助Jane在飛機中分辨鳥類 :+1: 從CIFAR-10資料集中，挑選所有鳥類和飛機的資料，並建立一個可以區分鳥類和飛機的神經網絡。 1 \u0026ndash; Building the dataset: 第一步: get the data in the right shape\n可以創建一個Dataset僅包含鳥類和飛機 (但資料集很小，我們只需建立索引，並讓len在資料集上工作即可。)\n實際上，它不一定是torch.utils.data.dataset.Dataset的子類\n# In[5]:\r# filter the data in cifar10 # and remap the labels # so they are contiguous\rlabel_map = {0: 0, 2: 1}\rclass_names = [\u0026#39;airplane\u0026#39;, \u0026#39;bird\u0026#39;]\rcifar2 = [(img, label_map[label])\rfor img, label in cifar10\rif label in [0, 2]]\rcifar2_val = [(img, label_map[label])\rfor img, label in cifar10_val\rif label in [0, 2]]\r# The \u0026#34;cifar2\u0026#34; object satisfies the basic requirements for a \u0026#34;Dataset\u0026#34;\r# that is, \u0026#39;__len__\u0026#39; and \u0026#39;__getitem__\u0026#39; are defined 這是一個聰明的捷徑，如果我們遇到限制，我們可能希望用適合的Dataset\n 2 \u0026ndash; A fully connected model  從ch5 得知它是\u0026rdquo; a tensor of features in, a tensor of features out \u0026quot; 圖像只是在空間配置中排列的一組數字 理論上，如果僅獲取圖像像素，並將其拉直成一個長的一維向量，我們可以將這些數字視為輸入特徵(如下圖)   tags: Treating our image as a 1D vector of values and training a fully connected classifier on it  從ch5中構建的模型開始，新模型將是nn.Linear:   具有3072個input features的模型 some number of hidden features, followed by an activation another nn.Linear that tapers the network down to (縮減到適當數量) an appropriate output number of features (2, for this use case)  # In[6]:\rimport torch.nn as nn\rn_out = 2\rmodel = nn.Sequential(\rnn.Linear(\r# 32×32×3：每個樣本3,072個輸入特徵\r3072, #input feature\r512, #hidden layer size\r# arbitrarily(任意) pick 512 hidden features\r),\rnn.Tanh(),\rnn.Linear(\r512,\rn_out, #output class:輸出類別\r)\r)  一個神經網絡需要至少一個隱藏層(of activations, so two modules）之間具有非線性 隱藏的特徵表示通過權重矩陣編碼的輸入之間的（學習）關係   3 \u0026ndash; Output of a classifier  make our network output a single scalar value（so n_out = 1） 將標籤轉換為浮點數（飛機為0.0，鳥為1.0）， 將其為MSELoss（the average of squared differences in the batch）當成目標為 這樣做會將問題轉化為回歸問題  output is categorical: it’s either a bird or an airplane (or something else if we had all 10 of the original classes). 簡單來說，就是當我們必須表示分類變量(categorical variable)時，應該切換到該變量的one-hot-encoding形式，例如[1, 0]飛機或[0, 1]鳥類（順序是任意的）。\n理想情況:\n   類別 飛機 鳥     預期輸出 torch.tensor([1.0, 0.0]) torch.tensor([0.0, 1.0]    tags: 關鍵的實現是\u0026ndash;可以將\u0026quot;輸出\u0026quot;解釋為\u0026quot;機率\u0026quot;：第一個輸入是“飛機”的機率，第二個輸入是“鳥”的機率 \u0026laquo;Casting the problem in terms of probabilities imposes a few extra constraints on the outputs of our network\u0026raquo;:\n 輸出的每個元素必須在[0.0, 1.0]範圍內（結果的機率不能小於0或大於1）。 輸出元素的總和必須為1.0（會出現兩種結果之一）。   4 \u0026ndash; Representing the output as probabilities(這裡會介紹softmax) Enforce in a differentiable way on a vector of numbers \u0026ndash;\u0026gt; softmax (differentiable)\n\nSoftmax is a function that takes a vector of values and produces another vector of the same dimension(它使用值的向量並生成另一個維度相同的向量), where the values satisfy the constraints we just listed to represent probabilities.\n# In[7]:\r# 取向量的元素，計算元素指數，然後將每個元素除以指數和\rdef softmax(x):\rreturn torch.exp(x) / torch.exp(x).sum()  Test it on an input vector:  # In[8]:\rx = torch.tensor([1.0, 2.0, 3.0])\rsoftmax(x) # Out[8]: tensor([0.0900, 0.2447, 0.6652])  As expected, it satisfies the constraints(約束) on probability:  # In[9]:\rsoftmax(x).sum() # Out[9]: tensor(1.)  softmax是monotone function: 輸入的較低值，對應於輸出中的較低值。 It’s not scale invariant(不變), in that the ratio between values is not preserved 學習過程將以值(value)具有適當比率的方式來驅動模型的參數     softmax input correspond to output     value lower \u0026lt;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt; lower   value higher \u0026lt;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt; higher    nn模塊使softmax可當成模塊。\n像往常一樣，輸入tensor可能具有額外的 batch 0th dimension，或俱有沿其編碼機率而沒有沿機率編碼的維度。\n因此nn.Softmax需要指定要應用softmax函數的尺寸：\n# In[10]:\rsoftmax = nn.Softmax(dim=1)\rx = torch.tensor([[1.0, 2.0, 3.0],\r[1.0, 2.0, 3.0]])\rsoftmax(x) # Out[10]: tensor([[0.0900, 0.2447, 0.6652],  [0.0900, 0.2447, 0.6652]]) We have two input vectors in two rows (just like when we work with batches), so we initialize nn.Softmax to operate along dimension 1.\n# In[11]:\rmodel = nn.Sequential(\rnn.Linear(3072, 512),\rnn.Tanh(),\rnn.Linear(512, 2),\rnn.Softmax(dim=1)) # 模型的末尾添加一個softmax  在訓練模型之前嘗試running module:  # In[12]:\r# 構建一個圖像\rimg, _ = cifar2[0] #a batch of one image, our bird\rplt.imshow(img.permute(1, 2, 0))\rplt.show() tags: CIFAR-10資料集中的一隻隨機鳥（正規化後）  為了call the model，我們需要使輸入具有正確的尺寸。 Our model expects 3,072 features in the input, and that nn works with data organized into batches along the zeroth dimension.  # In[13]:\r# 將3×32×32圖像轉換為1D tensor\r# 在第0位置添加額外的維度\rimg_batch = img.view(-1).unsqueeze(0) # In[14]:\r# invoke our model\rout = model(img_batch)\rout # Out[14]: tensor([[0.4784, 0.5216]], grad_fn=\u0026lt;SoftmaxBackward\u0026gt;) In our case, we need to take the max along the probability vector (not across batches), therefore, dimension 1:\n# In[15]:\r# 當提供維度時，torch.max返回最大值沿該維度的元素以及該值出現的index\r_, index = torch.max(out, dim=1)\rindex # Out[15]: tensor([1]) 小節:  通過將模型輸出轉換為輸出機率來調整模型輸出，以適應手頭的分類任務 針對輸入圖像running模型，並驗證plumbing works是否有效   5 \u0026ndash; A loss for classifying 使用了均方差（MSE）作為損失 可以用MSE並使輸出機率收斂於[0.0, 1.0]和[1.0, 0.0] 飛機的第一個機率高於第二個機率，而鳥類則相反\nWhat we need to maximize in this case is the probability associated with the correct class, out[class_index], where out is the output of softmax and class_index is a vector containing 0 for “airplane” and 1 for “bird” for each sample.\nThis quantity\u0026ndash;that is, the probability associated with the correct class\u0026ndash;is referred to as the likelihood (of our model’s parameters, given the data)\n簡單來說，我們希望損失函數在likelihood很低時非常高，likelihood之低，以至於替代方案的可能性更高。\n反之，當likelihood比其他替代方案高時，損失應該很小，而我們並沒有真正將機率提高到1。\n** negative log likelihood (NLL) \u0026ndash; a loss function that behaves that way\nNLL = - sum(log(out_i[c_i]))\r# the sum is taken over N samples # c_i is the correct class for sample i. tags: The NLL loss as a function of the predicted probabilities 上圖顯示，當為資料分配到低機率時，NLL增長到無窮大； 當機率大於0.5時，NLL以相當淺的速率降低。\n★ NLL以機率為輸入；因此，隨著likelihood的增加，其他機率必然會降低。\n綜上所述，我們的分類損失可以如下計算(For each sample in the batch)：\n Run the forward pass, 並從最後一（線性）層獲取輸出值。 計算他們的softmax，並獲得probabilities(機率)。 取對應於正確類別的預測機率(the likelihood of the parameters）。(supervised problem) 計算logarithm(對數)，在其前面打一個減號，然後將其添加到損失中。  ★ nn.NLLLoss class: 相對而言達到預期，它不會採用probabilities，而會將log probabilities tensor作為輸入。 計算NLL給定了一批資料輸入，當機率接近0時，取機率的對數是很棘手的。 解決方法是nn.LogSoftmax而不是nn.Softmax，這需要注意使計算數值穩定。\nmodel = nn.Sequential(\rnn.Linear(3072, 512),\rnn.Tanh(),\rnn.Linear(512, 2),\rnn.LogSoftmax(dim=1)) # instantiate our NLL loss\rloss = nn.NLLLoss() img, label = cifar2[0]\rout = model(img.view(-1).unsqueeze(0))\rloss(out, torch.tensor([label]))\r# first argument -- loss取\u0026#39;nn.LogSoftmax\u0026#39;一批的輸出\r# second argument -- a tensor of class indices (zeros and ones, in our case)\rtensor(0.6509, grad_fn=\u0026lt;NllLossBackward\u0026gt;) 使用交叉熵loss如何比MSE有所改善: 當預測偏離目標時，交叉熵loss有一定的斜率（在低損耗角落，正確的類別被分配了99.97％的預測機率）；而MSE在一開始就更早飽和，更重要的是，對於非常錯誤的預測也是如此。\n根本原因\u0026ndash;MSE的斜率太低，無法補償針對錯誤預測的softmax函數的 flatness。\n這就是為什麼針對機率的MSE不適用於分類的原因。\ntags: The cross entropy (left) and MSE between predicted probabilities and the target probability vector (right) as functions of the predicted scores\u0026ndash;that is, before the (log-) softmax  6 \u0026ndash; Training the classifier import torch\rimport torch.nn as nn\rmodel = nn.Sequential(\rnn.Linear(3072, 512),\rnn.Tanh(),\rnn.Linear(512, 2),\rnn.LogSoftmax(dim=1))\rlearning_rate = 1e-2\roptimizer = optim.SGD(model.parameters(), lr=learning_rate)\rloss_fn = nn.NLLLoss()\rn_epochs = 100\rfor epoch in range(n_epochs):\rfor img, label in cifar2:\rout = model(img.view(-1).unsqueeze(0))\rloss = loss_fn(out, torch.tensor([label]))\roptimizer.zero_grad()\rloss.backward()\roptimizer.step()\rprint(\u0026#34;Epoch: %d, Loss: %f\u0026#34; % (epoch, float(loss))) tags: 訓練循環：（A）在整個資料集中平均更新；（B）在每個樣本上更新模型；（C）平均迷你批次的更新 在微型批處理上估計的以下梯度（在整個數據集上估計的梯度較差）有助於收斂並防止優化過程陷入其在過程中遇到的局部最小值 tags: 整個資料集（light path）的平均梯度下降與隨機梯度下降的平均值，其中梯度是在隨機選取的minibatches上估算的 tags: 資料加載器通過使用資料集對單個資料項進行採樣來分配小批量 torch.utils.data模塊有一個class，可幫助在小型批次中改組和組織資料：DataLoader。 資料加載器的工作是從資料集中對微型批次進行採樣 從而可以靈活地選擇不同的採樣策略\n(一個常見的策略: 在每個時期對資料進行混洗後進行均勻採樣)\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\rshuffle=True)\r# At a minimum, the \u0026#39;DataLoader\u0026#39; constructor takes a Dataset object as input,\r# along with \u0026#39;batch_size\u0026#39; and a \u0026#39;shuffle\u0026#39; Boolean # that indicates whether the data needs to be shuffled at the beginning of each epoch # A \u0026#39;DataLoader\u0026#39; can be iterated over\r# so we can use it directly in the inner loop of our new training code\rimport torch\rimport torch.nn as nn\rtrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\rshuffle=True)\rmodel = nn.Sequential(\rnn.Linear(3072, 512),\rnn.Tanh(),\rnn.Linear(512, 2),\rnn.LogSoftmax(dim=1))\rlearning_rate = 1e-2\roptimizer = optim.SGD(model.parameters(), lr=learning_rate)\rloss_fn = nn.NLLLoss()\rn_epochs = 100\rfor epoch in range(n_epochs):\rfor imgs, labels in train_loader:\rbatch_size = imgs.shape[0]\routputs = model(imgs.view(batch_size, -1))\rloss = loss_fn(outputs, labels)\roptimizer.zero_grad()\rloss.backward()\roptimizer.step()\rprint(\u0026#34;Epoch: %d, Loss: %f\u0026#34; % (epoch, float(loss))) 在每個內部iteration中，imgs大小為64×3×32×32的tensor-即一小批包含64張（32×32）RGB圖像-而labels大小為64的tensor包含label索引\n# 訓練 Epoch: 0, Loss: 0.523478 Epoch: 1, Loss: 0.391083 Epoch: 2, Loss: 0.407412 Epoch: 3, Loss: 0.364203 ... Epoch: 96, Loss: 0.019537 Epoch: 97, Loss: 0.008973 Epoch: 98, Loss: 0.002607 Epoch: 99, Loss: 0.026200 看到loss以某種方式減少，但是不知道loss是否足夠低。 由於我們的目標是正確地為圖像分配類別，並且最好在一個獨立的資料集上進行。 因此我們可以根據正確類別的總數在驗證集上計算模型的準確性：\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\rshuffle=False)\rcorrect = 0\rtotal = 0\rwith torch.no_grad():\rfor imgs, labels in val_loader:\rbatch_size = imgs.shape[0]\routputs = model(imgs.view(batch_size, -1))\r_, predicted = torch.max(outputs, dim=1)\rtotal += labels.shape[0]\rcorrect += int((predicted == labels).sum())\rprint(\u0026#34;Accuracy: %f\u0026#34;, correct / total)\rAccuracy: 0.794000 # 相當任意的多層 (透過更多層來為模型添加一些亮點)\rmodel = nn.Sequential(\rnn.Linear(3072, 1024),\rnn.Tanh(),\rnn.Linear(1024, 512),\rnn.Tanh(),\rnn.Linear(512, 128),\rnn.Tanh(),\rnn.Linear(128, 2),\rnn.LogSoftmax(dim=1)) The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.\nnn.NLLoss: 實際上計算的是交叉熵，但對數機率預測為輸入nn.CrossEntropyLoss得分的地方（有時稱為logits）。 從技術上講，nn.NLLLoss是狄拉克分佈之間的交叉熵，將所有質量放在目標上，並通過對數機率輸入給出預測的分佈。\nTo add to the confusion, in information theory, up to normalization by sample size, this cross entropy can be interpreted as a negative log likelihood of the predicted distribution under the target distribution as an outcome.\n當我們的模型預測（softmax應用的）機率時，這兩個loss都是給定資料的模型參數的negative log likelihood\nmodel = nn.Sequential(\rnn.Linear(3072, 1024),\rnn.Tanh(),\rnn.Linear(1024, 512),\rnn.Tanh(),\rnn.Linear(512, 128),\rnn.Tanh(),\rnn.Linear(128, 2))\rloss_fn = nn.CrossEntropyLoss() # 刪除nn.LogSoftmax網絡的最後一層並把nn.CrossEntropyLoss當成損失 唯一的麻煩：模型輸出不會被解釋為機率（或對數機率）。 需要通過softmax傳遞輸出以獲得這些輸出。\n parameters() method of nn.Model: 一種快速的方法來確定多少個參數模型(用來提供參數給優化器) 我們可能希望將可訓練參數的數量與整個模型的大小區分開。  # In[7]:\rnumel_list = [p.numel() #要找出每個tensor實例中有多少個元素，我們可以調用該numel方法\rfor p in connected_model.parameters()\rif p.requires_grad == True\r#根據我們的用例，計算參數可能需要我們檢查參數是否也requires_grad設置True]\rsum(numel_list), numel_list # Out[7]: (3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2]) #370萬個參數，網路很龐大 # In[9]:\rnumel_list = [p.numel() for p in first_model.parameters()]\rsum(numel_list), numel_list # Out[9]: (1574402, [1572864, 512, 1024, 2]) 第一個模型中的參數數量大約是最新模型中參數的一半。\n從單個參數大小的列表中，開始知道是什麼原因： 第一個模塊具有150萬個參數 在我們的整個網絡中，我們具有1024個輸出功能，這導致第 一個線性模塊具有300萬個參數 知道線性層可以計算y = weight * x + bias 且如果x長度為3072（為簡單起見，不考慮批處理維度） 且y長度必須為1024 則weight tensor的bias大小必須為1024×3072，且大小必須為1024。\n正如我們先前所發現的，1024 * 3072 + 1024 = 3146752。我們可以直接驗證這些數量：\n# In[10]:\rlinear = nn.Linear(3072, 1024)\rlinear.weight.shape, linear.bias.shape # Out[10]: (torch.Size([1024, 3072]), torch.Size([1024]))  # 我們的神經網絡無法隨像素數量很好地縮放 Q : 如果我們有1024×1024 RGB圖像怎麼辦？ A: 那是310萬個輸入值。\n即使突然使用1024個hidden features（對於我們的分類器也不起作用），也將擁有超過30億個參數。\n使用32位浮點數，且已有12 GB的RAM，甚至還沒有達到第二層，更不用說計算和存儲漸變了。這只是不適合大多數當今的GPU。\n 7 \u0026ndash; The limits of going fully connected 獲取每個輸入值一樣，即RGB中的每個單個分量圖像-並將其與每個輸出要素的所有其他值進行線性組合。 一方面，我們允許圖像中任何像素與每個其他像素的組合可能與我們的任務相關。另一方面，由於我們將圖像視為數字的一個大向量，因此我們沒有利用相鄰像素或遙遠像素的相對位置。 tags: 使用帶有輸入圖像的完全連接的模塊：每個輸入像素彼此組合在一起，以在輸出中生成每個元素。 tags: Translation invariance, or the lack thereof, with fully connected layers(具有完全連接的層的平移不變性或缺少) 問題和網絡結構之間的不匹配，最終導致過度吻合了訓練資料，而不是學習模型要檢測的一般特徵。\n 7-3 結論 解決了一個簡單的分類問題：從資料集到模型，再到最小化訓練循環中的適當Loss\n作者發現他們的模型存在嚴重缺陷：一直將2D圖像視為1D資料。 同樣，我們沒有自然的方法來合併問題的平移不變性\n 下一章：學習如何利用圖像資料的2D性質來獲得更好的結果\n可以立即使用學到的知識來處理資料，而不會出現這種translation invariance\n","permalink":"https://hicbcb.github.io/blog/learning-from-images/","tags":["ML"],"title":"Reading Note: Telling birds from airplanes - Learning from images"},{"categories":["Learning Note"],"contents":"圖學筆記 - 基本名詞與openFramework結構 credit:　Mastering Openframeworks (e-BOOK)\n任何影像形成須包含兩種基本實體:物件(object) \u0026amp; 觀察者(也是相機擺放位置, camera)\n物件(object)通常由一組**頂點(vertex)**定義，大部分API會提供使用者幾組元件(primitive, 又稱基本物件)給使用者，這些元件通常可以由硬體快速顯示。\n常見元件 : 點、線段、多邊形、文字。\nOpenGL透過一組組頂點定義元件。\n也需考慮的因素:光源\nRaster (光柵) Raster: pixel的陣列 所有的繪圖系統都以raster(光柵)為基礎(以光柵形式產生) 每點對應到影像中的一個位置(或小區域)\nframe buffer 所有的pixel儲存到frame buffer(譯名是 \u0026lsquo;鏡頭緩衝區\u0026rsquo; ??)\n 註 : frame buffer算是一種記憶體區域(標穩記憶體的一部分) frame buffer的深度(depth):每個pixel所使用的位元個數，決定一個pixel的特性(例如: 顏色)。 frame buffer中pixel的總數 \u0026ndash;\u0026gt; resolution(解析度)，決定影像中可以看到的詳細程度。  Rasterization(描繪、光柵化) 定義: 將幾何實體圖形轉換成frame buffer的pixel值\nRendering 定義: 從2D或3D mesh的集合中創造圖(影像)的過程\nRay tracing 追蹤光源發出的光，並依此建一個呈像模型。\nMesh openframework 專案結構 資料夾:\n bin : 包含專案可執行的檔案 bin \u0026ndash;\u0026gt; data : 資料，包含圖、影片、聲音、文字檔等 src : 含C++ source code (程式檔main.cpp, testApp.h, testApp.cpp.)  CODE結構 main.cpp: 定義main()函數，裡面最重要的\u0026ndash;\u0026gt; 定義視窗size (創造一個視窗，可見的長、寬設定)\nofSetupOpenGL(\u0026amp;window, 1024, 768, OF_WINDOW) //OF_WINDOW代表你定義的視窗可以讓使用者在桌面移動或縮放這個視窗\r// 最後面參數可以改成OF_FULLSCREEN，讓視窗變全螢幕(1910 x 1024) (注意): openframework的物件(例如ofImage)來不能運作，因為路徑和其他變數還沒設定好。 盡量不要動到main.cpp，去testapp.cpp做你需要的操作\ntestApp.h 開始: #pragma once. (所有.h檔的起始) 下一行是: include \u0026quot;ofMain.h\u0026quot; \u0026ndash;\u0026gt; 它包含OpenFramework的core class和函數，宣告testApp繼承ofBaseApp的類別(class)\n","permalink":"https://hicbcb.github.io/blog/cg_basic/","tags":["ML"],"title":"Learning Note: Computer Graphic - basic structure and OpenFrameworks"}]